{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using k-nearest neighbours to classify handwritten digits (mnist) dataset\n",
    "\n",
    "\n",
    "In the following set of notebooks, we shall establish a pipeline to classify mnist data using a nearest neighbour approach. The objective is to understand the flow of Bluedata ML Ops pipeline. There are three key stages in developing the model:\n",
    "\n",
    "1. Clean and convert data into a CSV file.\n",
    "2. Apply dimensionality reduction on the training data\n",
    "3. Train a kNN model to classify the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import decomposition\n",
    "import pickle\n",
    "\n",
    "# nfs path is used as a common repository to store code, data and ML models\n",
    "nfs_path = \"/bd-fs-mnt/sj_ai_ml/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "### Convert raw format to a csv\n",
    "\n",
    "The [mnist dataset](http://yann.lecun.com/exdb/mnist/) is in a non-text format and we use the code from [this github repo](https://github.com/pjreddie/mnist-csv-png) to convert it into a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(imgf, labelf, outf, n):\n",
    "    f = open(imgf, \"rb\")\n",
    "    o = open(outf, \"w\")\n",
    "    l = open(labelf, \"rb\")\n",
    "\n",
    "    f.read(16)\n",
    "    l.read(8)\n",
    "    images = []\n",
    "\n",
    "    for i in range(n):\n",
    "        image = [ord(l.read(1))]\n",
    "        for j in range(28*28):\n",
    "            image.append(ord(f.read(1)))\n",
    "        images.append(image)\n",
    "\n",
    "    for image in images:\n",
    "        o.write(\",\".join(str(pix) for pix in image)+\"\\n\")\n",
    "    f.close()\n",
    "    o.close()\n",
    "    l.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert training data to csv\n",
    "convert( nfs_path + \"data/train-images\", nfs_path + \"data/train-labels\",\n",
    "        nfs_path + \"data/mnist_train.csv\", 60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert test data to csv\n",
    "convert(nfs_path + \"data/test-images\", nfs_path + \"data/test-labels\",\n",
    "        nfs_path + \"data/mnist_test.csv\", 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Now, we load the training and test data into memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.loadtxt(nfs_path + \"data/mnist_train.csv\", delimiter=\",\")\n",
    "test_data = np.loadtxt(nfs_path + \"data/mnist_test.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label, train_data = np.array_split(train_data, [1], axis = 1)\n",
    "test_label, test_data = np.array_split(test_data, [1], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data/255.\n",
    "test_data = test_data/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompose the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_fit_and_dump(n_dim, data, save_path):\n",
    "    pca = decomposition.PCA(n_components=n_dim)\n",
    "    pca.fit(data)\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(pca, f)\n",
    "    return pca\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now assess the decomposition to see the optimum number of features that would help balance the accuracy. For our example, we assume that 200 components with an explained variance of 96.67% are sufficient to get an accuracy good enough for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9667293139572224\n"
     ]
    }
   ],
   "source": [
    "pca = decomposition.PCA()\n",
    "pca.fit(train_data)\n",
    "ev = pca.explained_variance_ratio_\n",
    "print(np.cumsum(ev)[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_fit_and_dump(200, train_data, nfs_path + \"models/pca_mnist.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump data and labels\n",
    "\n",
    "We save the data and labels to the disk. We now have the decomposition PCA model fitted with training data, and the original CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(nfs_path + \"data/train_label\", train_label)\n",
    "np.save(nfs_path + \"data/train_data\", train_data)\n",
    "np.save(nfs_path + \"data/test_label\", test_label)\n",
    "np.save(nfs_path + \"data/test_data\", test_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
