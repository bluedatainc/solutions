{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In progress but operational.\n",
    "# This file downloads taxi trip data directly from the NYC Taxi Commission and trains a neural net iteratively.\n",
    "\n",
    "print(\"Importing libraries\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Activation,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "import urllib\n",
    "import sys\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from math import sqrt\n",
    "\n",
    "print(\"Done importing libraries\")\n",
    "\n",
    "# Specify months to train below. The available data ranges from 1/2009 to 12/2019.\n",
    "\n",
    "# listMonthsToTrain = [[2009, 1],\n",
    "#                  [2009, 2],\n",
    "#                  [2009, 3],\n",
    "#                  [2009, 4],\n",
    "#                  [2009, 5],\n",
    "#                  [2009, 6]]\n",
    "\n",
    "listMonthsToTrain = [[x,y] for x in list(range(2009, 2020)) for y in list(range(1, 13))]\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "baseDataUrl = \"https://s3.amazonaws.com/nyc-tlc/trip+data/\"\n",
    "\n",
    "trainSetSize = 0\n",
    "\n",
    "modelDirectory = '10yrdatasetchecknames'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Start time \n",
    "# print(\"Start time: \", datetime.datetime.now())\n",
    "\n",
    "\n",
    "def strAppendZero(month):\n",
    "    if (month < 10):\n",
    "        return \"0\" + str(month)\n",
    "    else:\n",
    "        return str(month)\n",
    "\n",
    "def taxiFilePath(year, month, extension):\n",
    "    year = str(year)\n",
    "    month = strAppendZero(month)\n",
    "    return ProjectRepo(\"data/originals/\" + year + \"/\" + month + \"/\" + \"yellow_tripdata_\" + year + \"-\" + month + \".\" + extension)\n",
    "\n",
    "def fileName(year, month, extension):\n",
    "    year = str(year)\n",
    "    month = strAppendZero(month)\n",
    "    return \"yellow_tripdata_\" + year + \"-\" + month + \".\" + extension\n",
    "\n",
    "def taxiFileParentPath(year, month):\n",
    "    year = str(year)\n",
    "    month = strAppendZero(month)\n",
    "    return ProjectRepo(\"data/originals/\" + year + \"/\" + month)\n",
    "\n",
    "# Project repo path function\n",
    "def ProjectRepo(path):\n",
    "    ProjectRepo = \"/bd-fs-mnt/project_repo\"\n",
    "    return str(ProjectRepo + '/' + path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get full name of the dataframe column by appending the database name to the beginning (a vestige from working with Hive) \n",
    "def fullName(colName):\n",
    "    return dbName + '.' + colName\n",
    "\n",
    "# Downloads data into the Project Repo if not present, then returns a dataframe containing that data.\n",
    "def downloadDataDf(year, month):\n",
    "    if (not os.path.exists(taxiFilePath(year, month, \"csv\"))):\n",
    "        if (os.path.exists(taxiFilePath(year, month, \"zip\"))):\n",
    "            with zipfile.ZipFile(taxiFilePath(year, month, \"zip\"), 'r') as zip_ref:\n",
    "                zip_ref.extractall(Path(taxiFilePath(year, month, \"zip\")).parent)\n",
    "        else:\n",
    "            url = baseDataUrl + fileName(year, month, \"csv\")\n",
    "            proxy = urllib.request.ProxyHandler({'https': 'web-proxy.corp.hpecorp.net:8080'})\n",
    "            opener = urllib.request.build_opener(proxy)\n",
    "            urllib.request.install_opener(opener)            \n",
    "            if not os.path.isdir(taxiFileParentPath(year, month)):\n",
    "                os.makedirs(taxiFileParentPath(year, month))            \n",
    "            urllib.request.urlretrieve (url, taxiFilePath(year, month, \"csv\") + \"-downloadInProgress\")\n",
    "            os.rename(taxiFilePath(year, month, \"csv\") + \"-downloadInProgress\", taxiFilePath(year, month, \"csv\"))\n",
    "            \n",
    "    if (year == 2016 and month >= 7):\n",
    "        df = pd.read_csv(taxiFilePath(year, month, \"csv\"), skiprows=1, names=['VendorID','tpep_pickup_datetime','tpep_dropoff_datetime','passenger_count','trip_distance','RatecodeID','store_and_fwd_flag','PULocationID','DOLocationID','payment_type','fare_amount','extra','mta_tax','tip_amount','tolls_amount','improvement_surcharge','total_amount','blank1','blank2'])\n",
    "    else:\n",
    "        df = pd.read_csv(taxiFilePath(year, month, \"csv\"), warn_bad_lines=True, error_bad_lines=False)\n",
    "#     if 'congestion_surcharge' in df.columns:\n",
    "#         del df['congestion_surcharge']\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    for str in ['vendor_name', 'passenger_count', 'rate_code', 'store_and_forward', 'payment_type', 'fare_amt', 'surcharge', 'mta_tax', 'tip_amt', 'tolls_amt', 'total_amt']:\n",
    "        if str in df.columns:\n",
    "            del df[str]\n",
    "    for str in ['vendor_id', 'passenger_count', 'store_and_fwd_flag', 'fare_amount', 'surcharge', 'tip_amount', 'tolls_amount', 'total_amount', 'congestion_surcharge', 'improvement_surcharge']:\n",
    "        if str in df.columns:\n",
    "            del df[str]\n",
    "    df = df.add_prefix('pqyellowtaxi.')\n",
    "    return df\n",
    "\n",
    "def mergeData(df, lookup):\n",
    "    if fullName('pulocationid') in df.columns:\n",
    "        df = pd.merge(df, dflook[[lookupDbName + '.location_i', lookupDbName + '.long', lookupDbName + '.lat']], how='left', left_on=dbName + '.pulocationid', right_on=lookupDbName + '.location_i')\n",
    "        df.rename(columns = {(lookupDbName + '.long'):(dbName + '.startstationlongitude')}, inplace = True)\n",
    "        df.rename(columns = {(lookupDbName + '.lat'):(dbName + '.startstationlatitude')}, inplace = True)\n",
    "        df = pd.merge(df, dflook[[lookupDbName + '.location_i', lookupDbName + '.long', lookupDbName + '.lat']], how='left', left_on=dbName + '.dolocationid', right_on=lookupDbName + '.location_i')\n",
    "        df.rename(columns = {(lookupDbName + '.long'):(dbName + '.endstationlongitude')}, inplace = True)\n",
    "        df.rename(columns = {(lookupDbName + '.lat'):(dbName + '.endstationlatitude')}, inplace = True)\n",
    "    else:\n",
    "        if fullName('pickup_longitude') in df.columns:\n",
    "            df.rename(columns = {(dbName + '.pickup_longitude'):(dbName + '.startstationlongitude')}, inplace = True)\n",
    "            df.rename(columns = {(dbName + '.pickup_latitude'):(dbName + '.startstationlatitude')}, inplace = True)\n",
    "            df.rename(columns = {(dbName + '.dropoff_longitude'):(dbName + '.endstationlongitude')}, inplace = True)\n",
    "            df.rename(columns = {(dbName + '.dropoff_latitude'):(dbName + '.endstationlatitude')}, inplace = True)\n",
    "        elif fullName('start_lon') in df.columns:\n",
    "            df.rename(columns = {(dbName + '.start_lon'):(dbName + '.startstationlongitude')}, inplace = True)\n",
    "            df.rename(columns = {(dbName + '.start_lat'):(dbName + '.startstationlatitude')}, inplace = True)\n",
    "            df.rename(columns = {(dbName + '.end_lon'):(dbName + '.endstationlongitude')}, inplace = True)\n",
    "            df.rename(columns = {(dbName + '.end_lat'):(dbName + '.endstationlatitude')}, inplace = True)\n",
    "        if fullName('trip_pickup_datetime') in df.columns:\n",
    "            df.rename(columns = {(dbName + '.trip_pickup_datetime'):(dbName + '.tpep_pickup_datetime')}, inplace = True)\n",
    "            df.rename(columns = {(dbName + '.trip_dropoff_datetime'):(dbName + '.tpep_dropoff_datetime')}, inplace = True)\n",
    "        elif fullName('pickup_datetime') in df.columns:\n",
    "            df.rename(columns = {(dbName + '.pickup_datetime'):(dbName + '.tpep_pickup_datetime')}, inplace = True)\n",
    "            df.rename(columns = {(dbName + '.dropoff_datetime'):(dbName + '.tpep_dropoff_datetime')}, inplace = True)\n",
    "    return df\n",
    "    \n",
    "\n",
    "    \n",
    "def generateFeatures(df):\n",
    "#     print(df.head(5))    \n",
    "    df[dbName + '.tpep_pickup_datetime'] = pd.to_datetime(df[dbName + '.tpep_pickup_datetime'])\n",
    "    df[dbName + '.tpep_dropoff_datetime'] = pd.to_datetime(df[dbName + '.tpep_dropoff_datetime'])\n",
    "    df[fullName('duration')] = (df[fullName(\"tpep_dropoff_datetime\")] - df[fullName(\"tpep_pickup_datetime\")]).dt.total_seconds()\n",
    "\n",
    "    df[fullName(\"weekday\")] = (df[fullName('tpep_pickup_datetime')].dt.dayofweek < 5).astype(float)\n",
    "    df[fullName(\"hour\")] = df[fullName('tpep_pickup_datetime')].dt.hour\n",
    "    df[fullName(\"work\")] = (df[fullName('weekday')] == 1) & (df[fullName(\"hour\")] >= 8) & (df[fullName(\"hour\")] < 18)\n",
    "    return df\n",
    "    \n",
    "def removeOutliers(df):\n",
    "    df = df[df[fullName('duration')] > 20]\n",
    "    df = df[df[fullName('duration')] < 10800]\n",
    "    df = df[df[fullName('trip_distance')] > 0]\n",
    "    df = df[df[fullName('trip_distance')] < 150]\n",
    "    return df\n",
    "\n",
    "dbName = \"pqyellowtaxi\"\n",
    "lookupDbName = \"pqlookup\"\n",
    "dflook = pd.read_csv(ProjectRepo('data/lookup-ipyheader.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for step in range(0, len(listMonthsToTrain)):   \n",
    "    year, month = listMonthsToTrain[step]\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Begin step \" + str(step) + \": year\" + str(year) + \" month\" + str(month))\n",
    "    \n",
    "    print(\"Reading in data\" + \" step \" + str(step))\n",
    "    # ==================================================================\n",
    "\n",
    "#     if (os.path.exists(ProjectRepo('data/originalsdf/' + fileName(year, month, 'pkl')))):\n",
    "#         dataset = pd.read_pickle(ProjectRepo('data/originalsdf/' + fileName(year, month, 'pkl')))\n",
    "#     else:\n",
    "    df = downloadDataDf(year, month)\n",
    "\n",
    "\n",
    "    # ==================================================================\n",
    "\n",
    "    print(\"Done reading in data, start data cleaning\" + \" step \" + str(step))\n",
    "    print(\"Dataset size before cleaning\" + \" step \" + str(step) + \": \" + str(len(df)))\n",
    "\n",
    "\n",
    "    print(\"merge\")\n",
    "    df = mergeData(df, dflook)\n",
    "\n",
    "    print(\"generateFeatures\")\n",
    "    df = generateFeatures(df)\n",
    "\n",
    "    print(\"removeOutliers\")\n",
    "    df = removeOutliers(df)\n",
    "\n",
    "#     print(sys.getsizeof(df))\n",
    "\n",
    "\n",
    "    cols = [fullName('work'), fullName('startstationlatitude'), fullName('startstationlongitude'), fullName('endstationlatitude'), fullName('endstationlongitude'), fullName('trip_distance'), fullName('weekday'), fullName('hour'), fullName('duration')]\n",
    "    dataset = df[cols]\n",
    "    dataset = dataset.dropna(how='any',axis=0)\n",
    "\n",
    "#     dataset.to_pickle(ProjectRepo('data/originalsdf/' + fileName(year, month, 'pkl')))\n",
    "    del df\n",
    "\n",
    "    X = dataset.iloc[:, 0:(len(cols) - 1)].values\n",
    "    y = dataset.iloc[:, (len(cols) - 1)].values\n",
    "    X = X.copy()\n",
    "    y = y.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Dataset size after cleaning\" + \" step \" + str(step) + \": \" + str(len(dataset)))\n",
    "    trainSetSize = trainSetSize + len(dataset)\n",
    "    print(\"Cumulative data size read in all steps up including step \" + str(step) + \": \" + str(trainSetSize))\n",
    "    print(\"Done cleaning data\" + \" step \" + str(step))\n",
    "\n",
    "    del dataset\n",
    "\n",
    "    print(\"Training...\" + \" step \" + str(step))\n",
    "\n",
    "\n",
    "\n",
    "    X_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    if (step == 0):\n",
    "        if (not os.path.exists(ProjectRepo('models/' + modelDirectory))):\n",
    "            os.mkdir(ProjectRepo('models/' + modelDirectory))\n",
    "        input_layer = Input(shape=(X.shape[1],))\n",
    "        dense_layer_1 = Dense(100, activation='relu')(input_layer)\n",
    "        dense_layer_2 = Dense(50, activation='relu')(dense_layer_1)\n",
    "        dense_layer_3 = Dense(25, activation='relu')(dense_layer_2)\n",
    "        output = Dense(1)(dense_layer_3)\n",
    "        model = Model(inputs=input_layer, outputs=output)\n",
    "        model.compile(loss=\"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])        \n",
    "    else:\n",
    "        prevModelPath = 'models/' + modelDirectory + '/' + str(step - 1) + '_tf'\n",
    "        model = load_model(ProjectRepo(prevModelPath))\n",
    "\n",
    "\n",
    "    print(\"Step \" + str(step) + \" begin training time: \", datetime.datetime.now())\n",
    "#     history = model.fit(X_train, y_train, batch_size=256, epochs=1, verbose=1, validation_split=0.2)\n",
    "    history_callback = model.fit(X_train, y_train, batch_size=256, epochs=1, verbose=1, validation_split=0.2)\n",
    "\n",
    "    loss_history = history_callback.history[\"loss\"]\n",
    "\n",
    "    lossHistoryDirPath = 'models/' + modelDirectory + '/' + 'history'\n",
    "    lossHistoryFilePath = lossHistoryDirPath + '/' + str(step) + '.txt'\n",
    "    \n",
    "#     if (not os.path.exists(ProjectRepo(lossHistoryDirPath))):\n",
    "#         os.mkdir(ProjectRepo(lossHistoryDirPath))\n",
    "        \n",
    "#     numpy_loss_history = np.array(loss_history)\n",
    "#     np.savetxt(ProjectRepo(lossHistoryFilePath), numpy_loss_history, delimiter=\",\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = y_pred.clip(min=0)\n",
    "\n",
    "    # print(np.sqrt(mean_squared_error(y_test,y_pred)))\n",
    "    print(\"Step \" + str(step) + \"metrics\")\n",
    "    print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "    print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "    print('Root Mean Squared Log Error:', np.sqrt(mean_squared_log_error( y_test, y_pred)))\n",
    "    print()\n",
    "    # Finish time\n",
    "    print(\"Step \" + str(step) + \" end training time: \", datetime.datetime.now())\n",
    "\n",
    "\n",
    "    if (not os.path.exists(ProjectRepo('models/' + modelDirectory))):\n",
    "        os.mkdir(ProjectRepo('models/' + modelDirectory + '/' + str(step) + '_tf'))\n",
    "    \n",
    "    modelPath = 'models/' + modelDirectory + '/' + str(step) + '_tf'\n",
    "    \n",
    "    model.save(ProjectRepo(modelPath))\n",
    "    del model\n",
    "    del X_train\n",
    "    del X_test\n",
    "    del y_train\n",
    "    del y_test\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
