{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KubeDirector ML Pipeline Demo (NYC Taxi Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisite: Please ensure `lookup-ipyheader.csv` and `small_yellow_tripdata_2009-01.csv` are loaded in your Project Repo under the data folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cluster        ML Engine\n",
      "----------------------  -----------\n",
      "trainingengineinstance  python\n"
     ]
    }
   ],
   "source": [
    "%attachments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History URL: http://kdss-2cd6v-0.kdhs-858ph.thetestingtenant.svc.cluster.local:10001/history/2\n"
     ]
    }
   ],
   "source": [
    "%%trainingengineinstance\n",
    "\n",
    "# --------------------------------------\n",
    "# If smallDemoDataset is False, the full dataset will be downloaded from s3://bd-kartik/testingalt_yellow_tripdata_2009-01.csv. Only January 2009 data is available here, more data can be found by uncommenting the lines that contain this url: https://s3.amazonaws.com/nyc-tlc/trip+data/\n",
    "# If smallDemoDataset is True, a sample dataset will be loaded from the project repo at data/small_yellow_tripdata_2009-01.csv\n",
    "smallDemoDataset = True\n",
    "\n",
    "numEpochs = 1\n",
    "modelDirectory = 'testmodel'\n",
    "# --------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "print(\"Importing libraries\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Activation,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "import urllib\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from math import sqrt\n",
    "\n",
    "print(\"Done importing libraries\")\n",
    "\n",
    "# Specify months to train below. The available data ranges from 1/2009 to 12/2019.\n",
    "\n",
    "listMonthsToTrain = [[2009, 1]]\n",
    "\n",
    "# listMonthsToTrain = [[x,y] for x in list(range(2009, 2020)) for y in list(range(1, 13))]\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "trainSetSize = 0\n",
    "\n",
    "\n",
    "\n",
    "def strAppendZero(month):\n",
    "    if (month < 10):\n",
    "        return \"0\" + str(month)\n",
    "    else:\n",
    "        return str(month)\n",
    "\n",
    "def taxiFilePath(year, month, extension):\n",
    "    year = str(year)\n",
    "    month = strAppendZero(month)\n",
    "    print(ProjectRepo(\"data/originals/\" + year + \"/\" + month + \"/\" + \"yellow_tripdata_\" + year + \"-\" + month + \".\" + extension))\n",
    "    return ProjectRepo(\"data/originals/\" + year + \"/\" + month + \"/\" + \"yellow_tripdata_\" + year + \"-\" + month + \".\" + extension)\n",
    "\n",
    "def fileName(year, month, extension):\n",
    "    year = str(year)\n",
    "    month = strAppendZero(month)\n",
    "    return \"yellow_tripdata_\" + year + \"-\" + month + \".\" + extension\n",
    "\n",
    "def taxiFileParentPath(year, month):\n",
    "    year = str(year)\n",
    "    month = strAppendZero(month)\n",
    "    return ProjectRepo(\"data/originals/\" + year + \"/\" + month)\n",
    "\n",
    "# Project repo path function\n",
    "def ProjectRepo(path):\n",
    "    ProjectRepo = \"/bd-fs-mnt/project_repo\"\n",
    "    return str(ProjectRepo + '/' + path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get full name of the dataframe column by appending the database name to the beginning (a vestige from working with Hive) \n",
    "def fullName(colName):\n",
    "    return dbName + '.' + colName\n",
    "\n",
    "# Downloads data into the Project Repo if not present, then returns a dataframe containing that data.\n",
    "def downloadDataDf(year, month):\n",
    "    if (not os.path.exists(taxiFilePath(year, month, \"csv\"))):\n",
    "        try:\n",
    "            if not os.path.isdir(taxiFileParentPath(year, month)):\n",
    "                os.makedirs(taxiFileParentPath(year, month))   \n",
    "                \n",
    "            url = \"s3://bd-kartik/testingalt_yellow_tripdata_2009-01.csv\"\n",
    "                \n",
    "# Uncomment the lines below to download data from the NYC Taxi Commission's S3 bucket                \n",
    "#             url = \"https://s3.amazonaws.com/nyc-tlc/trip+data/\" + fileName(year, month, \"csv\")\n",
    "#             proxy = urllib.request.ProxyHandler({'https': 'web-proxy.corp.hpecorp.net:8080'})\n",
    "#             opener = urllib.request.build_opener(proxy)\n",
    "#             urllib.request.install_opener(opener)      \n",
    "\n",
    "    \n",
    "            urllib.request.urlretrieve (url, taxiFilePath(year, month, \"csv\") + \"-downloadInProgress\")\n",
    "            os.rename(taxiFilePath(year, month, \"csv\") + \"-downloadInProgress\", taxiFilePath(year, month, \"csv\"))\n",
    "        except:\n",
    "            raise AssertionError(\"Error downloading dataset from S3\")\n",
    "            \n",
    "    if (year == 2016 and month >= 7):\n",
    "        df = pd.read_csv(taxiFilePath(year, month, \"csv\"), skiprows=1, names=['VendorID','tpep_pickup_datetime','tpep_dropoff_datetime','passenger_count','trip_distance','RatecodeID','store_and_fwd_flag','PULocationID','DOLocationID','payment_type','fare_amount','extra','mta_tax','tip_amount','tolls_amount','improvement_surcharge','total_amount','blank1','blank2'])\n",
    "    else:\n",
    "        df = pd.read_csv(taxiFilePath(year, month, \"csv\"), warn_bad_lines=True, error_bad_lines=False)\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    df.columns = df.columns.str.replace(' ', '')\n",
    "    for str in ['vendor_name', 'passenger_count', 'rate_code', 'store_and_forward', 'payment_type', 'fare_amt', 'surcharge', 'mta_tax', 'tip_amt', 'tolls_amt', 'total_amt']:\n",
    "        if str in df.columns:\n",
    "            del df[str]\n",
    "    for str in ['vendor_id', 'passenger_count', 'store_and_fwd_flag', 'fare_amount', 'surcharge', 'tip_amount', 'tolls_amount', 'total_amount', 'congestion_surcharge', 'improvement_surcharge']:\n",
    "        if str in df.columns:\n",
    "            del df[str]\n",
    "    df = df.add_prefix('pqyellowtaxi.')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def downloadDemoDf(year, month):\n",
    "    try:\n",
    "        df = pd.read_csv(ProjectRepo('data/small_yellow_tripdata_2009-01.csv'), warn_bad_lines=True, error_bad_lines=False)\n",
    "    except:\n",
    "        raise AssertionError(\"Error loading dataset from Project Repo\")\n",
    "\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    for str in ['vendor_name', 'passenger_count', 'rate_code', 'store_and_forward', 'payment_type', 'fare_amt', 'surcharge', 'mta_tax', 'tip_amt', 'tolls_amt', 'total_amt']:\n",
    "        if str in df.columns:\n",
    "            del df[str]\n",
    "    for str in ['vendor_id', 'passenger_count', 'store_and_fwd_flag', 'fare_amount', 'surcharge', 'tip_amount', 'tolls_amount', 'total_amount', 'congestion_surcharge', 'improvement_surcharge']:\n",
    "        if str in df.columns:\n",
    "            del df[str]\n",
    "    df = df.add_prefix('pqyellowtaxi.')\n",
    "    return df\n",
    "    \n",
    "\n",
    "def mergeData(df, lookup):\n",
    "    try:\n",
    "        if fullName('pulocationid') in df.columns:\n",
    "            df = pd.merge(df, dflook[[lookupDbName + '.location_i', lookupDbName + '.long', lookupDbName + '.lat']], how='left', left_on=dbName + '.pulocationid', right_on=lookupDbName + '.location_i')\n",
    "            df.rename(columns = {(lookupDbName + '.long'):(dbName + '.startstationlongitude')}, inplace = True)\n",
    "            df.rename(columns = {(lookupDbName + '.lat'):(dbName + '.startstationlatitude')}, inplace = True)\n",
    "            df = pd.merge(df, dflook[[lookupDbName + '.location_i', lookupDbName + '.long', lookupDbName + '.lat']], how='left', left_on=dbName + '.dolocationid', right_on=lookupDbName + '.location_i')\n",
    "            df.rename(columns = {(lookupDbName + '.long'):(dbName + '.endstationlongitude')}, inplace = True)\n",
    "            df.rename(columns = {(lookupDbName + '.lat'):(dbName + '.endstationlatitude')}, inplace = True)\n",
    "        else:\n",
    "            if fullName('pickup_longitude') in df.columns:\n",
    "                df.rename(columns = {(dbName + '.pickup_longitude'):(dbName + '.startstationlongitude')}, inplace = True)\n",
    "                df.rename(columns = {(dbName + '.pickup_latitude'):(dbName + '.startstationlatitude')}, inplace = True)\n",
    "                df.rename(columns = {(dbName + '.dropoff_longitude'):(dbName + '.endstationlongitude')}, inplace = True)\n",
    "                df.rename(columns = {(dbName + '.dropoff_latitude'):(dbName + '.endstationlatitude')}, inplace = True)\n",
    "            elif fullName('start_lon') in df.columns:\n",
    "                df.rename(columns = {(dbName + '.start_lon'):(dbName + '.startstationlongitude')}, inplace = True)\n",
    "                df.rename(columns = {(dbName + '.start_lat'):(dbName + '.startstationlatitude')}, inplace = True)\n",
    "                df.rename(columns = {(dbName + '.end_lon'):(dbName + '.endstationlongitude')}, inplace = True)\n",
    "                df.rename(columns = {(dbName + '.end_lat'):(dbName + '.endstationlatitude')}, inplace = True)\n",
    "            if fullName('trip_pickup_datetime') in df.columns:\n",
    "                df.rename(columns = {(dbName + '.trip_pickup_datetime'):(dbName + '.tpep_pickup_datetime')}, inplace = True)\n",
    "                df.rename(columns = {(dbName + '.trip_dropoff_datetime'):(dbName + '.tpep_dropoff_datetime')}, inplace = True)\n",
    "            elif fullName('pickup_datetime') in df.columns:\n",
    "                df.rename(columns = {(dbName + '.pickup_datetime'):(dbName + '.tpep_pickup_datetime')}, inplace = True)\n",
    "                df.rename(columns = {(dbName + '.dropoff_datetime'):(dbName + '.tpep_dropoff_datetime')}, inplace = True)\n",
    "        return df\n",
    "    except:\n",
    "        raise AssertionError(\"Error merging data, please verify column names have not been modified.\")\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "def generateFeatures(df):\n",
    "    df[dbName + '.tpep_pickup_datetime'] = pd.to_datetime(df[dbName + '.tpep_pickup_datetime'])\n",
    "    df[dbName + '.tpep_dropoff_datetime'] = pd.to_datetime(df[dbName + '.tpep_dropoff_datetime'])\n",
    "    df[fullName('duration')] = (df[fullName(\"tpep_dropoff_datetime\")] - df[fullName(\"tpep_pickup_datetime\")]).dt.total_seconds()\n",
    "\n",
    "    df[fullName(\"weekday\")] = (df[fullName('tpep_pickup_datetime')].dt.dayofweek < 5).astype(float)\n",
    "    df[fullName(\"hour\")] = df[fullName('tpep_pickup_datetime')].dt.hour\n",
    "    df[fullName(\"work\")] = (df[fullName('weekday')] == 1) & (df[fullName(\"hour\")] >= 8) & (df[fullName(\"hour\")] < 18)\n",
    "    return df\n",
    "    \n",
    "def removeOutliers(df):\n",
    "    df = df[df[fullName('duration')] > 20]\n",
    "    df = df[df[fullName('duration')] < 10800]\n",
    "    df = df[df[fullName('trip_distance')] > 0]\n",
    "    df = df[df[fullName('trip_distance')] < 150]\n",
    "    return df\n",
    "\n",
    "dbName = \"pqyellowtaxi\"\n",
    "lookupDbName = \"pqlookup\"\n",
    "try:\n",
    "    dflook = pd.read_csv(ProjectRepo('data/lookup-ipyheader.csv'))\n",
    "except:\n",
    "    raise AssertionError(\"Please ensure lookup-ipyheader.csv is in your Project Repo under the data folder.\")\n",
    "    \n",
    "\n",
    "for step in range(0, len(listMonthsToTrain)):   \n",
    "    year, month = listMonthsToTrain[step]\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Begin step \" + str(step) + \": year\" + str(year) + \" month\" + str(month))\n",
    "    \n",
    "    print(\"Reading in data\" + \" step \" + str(step))\n",
    "    # ==================================================================\n",
    "\n",
    "\n",
    "    if smallDemoDataset is True:\n",
    "        df = downloadDemoDf(year, month)\n",
    "    else:\n",
    "        df = downloadDataDf(year, month)\n",
    "        \n",
    "\n",
    "    # ==================================================================\n",
    "\n",
    "    print(\"Done reading in data, start data cleaning\" + \" step \" + str(step))\n",
    "    print(\"Dataset size before cleaning\" + \" step \" + str(step) + \": \" + str(len(df)))\n",
    "\n",
    "\n",
    "    print(\"merge\")\n",
    "    df = mergeData(df, dflook)\n",
    "\n",
    "    print(\"generateFeatures\")\n",
    "    df = generateFeatures(df)\n",
    "\n",
    "    print(\"removeOutliers\")\n",
    "    df = removeOutliers(df)\n",
    "\n",
    "\n",
    "    cols = [fullName('work'), fullName('startstationlatitude'), fullName('startstationlongitude'), fullName('endstationlatitude'), fullName('endstationlongitude'), fullName('trip_distance'), fullName('weekday'), fullName('hour'), fullName('duration')]\n",
    "    dataset = df[cols]\n",
    "    dataset = dataset.dropna(how='any',axis=0)\n",
    "\n",
    "    del df\n",
    "\n",
    "    X = dataset.iloc[:, 0:(len(cols) - 1)].values\n",
    "    y = dataset.iloc[:, (len(cols) - 1)].values\n",
    "    X = X.copy()\n",
    "    y = y.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Dataset size after cleaning\" + \" step \" + str(step) + \": \" + str(len(dataset)))\n",
    "    trainSetSize = trainSetSize + len(dataset)\n",
    "    print(\"Cumulative data size read in all steps up including step \" + str(step) + \": \" + str(trainSetSize))\n",
    "    print(\"Done cleaning data\" + \" step \" + str(step))\n",
    "\n",
    "    del dataset\n",
    "\n",
    "    print(\"Training...\" + \" step \" + str(step))\n",
    "\n",
    "\n",
    "\n",
    "    X_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    \n",
    "    if (step == 0):\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.transform(X_test)\n",
    "        if (not os.path.exists(ProjectRepo('models/' + modelDirectory))):\n",
    "            os.mkdir(ProjectRepo('models/' + modelDirectory))\n",
    "        input_layer = Input(shape=(X.shape[1],))\n",
    "        dense_layer_1 = Dense(100, activation='relu')(input_layer)\n",
    "        dense_layer_2 = Dense(50, activation='relu')(dense_layer_1)\n",
    "        dense_layer_3 = Dense(25, activation='relu')(dense_layer_2)\n",
    "        output = Dense(1)(dense_layer_3)\n",
    "        model = Model(inputs=input_layer, outputs=output)\n",
    "        model.compile(loss=\"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "        pickle.dump(sc, open(ProjectRepo('models/' + modelDirectory + '/scaler.pkl'),'wb'))\n",
    "    else:\n",
    "#         If step is not 0, then model training should continue based on the previous step's model.\n",
    "        sc = pickle.load(open(ProjectRepo('models/' + modelDirectory + '/scaler.pkl'),'rb'))\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.transform(X_test)\n",
    "        prevModelPath = 'models/' + modelDirectory + '/' + str(step - 1) + '_tf'\n",
    "        model = load_model(ProjectRepo(prevModelPath))\n",
    "\n",
    "\n",
    "    print(\"Step \" + str(step) + \" begin training time: \", datetime.datetime.now())\n",
    "    history_callback = model.fit(X_train, y_train, batch_size=256, epochs=numEpochs, verbose=1, validation_split=0.2)\n",
    "\n",
    "    loss_history = history_callback.history[\"loss\"]\n",
    "\n",
    "    lossHistoryDirPath = 'models/' + modelDirectory + '/' + 'history'\n",
    "    lossHistoryFilePath = lossHistoryDirPath + '/' + str(step) + '.txt'\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = y_pred.clip(min=0)\n",
    "\n",
    "    print(\"Step \" + str(step) + \"metrics\")\n",
    "    print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "    print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "    print('Root Mean Squared Log Error:', np.sqrt(mean_squared_log_error( y_test, y_pred)))\n",
    "    print()\n",
    "    # Finish time\n",
    "    print(\"Step \" + str(step) + \" end training time: \", datetime.datetime.now())\n",
    "\n",
    "\n",
    "    if (not os.path.exists(ProjectRepo('models/' + modelDirectory))):\n",
    "        os.mkdir(ProjectRepo('models/' + modelDirectory + '/' + str(step) + '_tf'))\n",
    "    \n",
    "    modelPath = 'models/' + modelDirectory + '/' + str(step) + '_tf'\n",
    "    \n",
    "    model.save(ProjectRepo(modelPath))\n",
    "    del model\n",
    "    del X_train\n",
    "    del X_test\n",
    "    del y_train\n",
    "    del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status: Finished\n",
      "Importing libraries\n",
      "Done importing libraries\n",
      "Begin step 0: year2009 month1\n",
      "Reading in data step 0\n",
      "Done reading in data, start data cleaning step 0\n",
      "Dataset size before cleaning step 0: 14998\n",
      "merge\n",
      "generateFeatures\n",
      "removeOutliers\n",
      "Dataset size after cleaning step 0: 14546\n",
      "Cumulative data size read in all steps up including step 0: 14546\n",
      "Done cleaning data step 0\n",
      "Training... step 0\n",
      "Step 0 begin training time:  2020-08-10 07:41:42.350525\n",
      "Train on 9308 samples, validate on 2328 samples\n",
      "Epoch 1/2\n",
      "256/9308 [..............................] - ETA: 16s - loss: 496655.8125 - mean_squared_error: 496655.8125\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "5632/9308 [=================>............] - ETA: 0s - loss: 649572.3679 - mean_squared_error: 649572.3750 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "9308/9308 [==============================] - 1s 69us/sample - loss: 656424.1428 - mean_squared_error: 656424.1875 - val_loss: 611815.8397 - val_mean_squared_error: 611815.9375\n",
      "Epoch 2/2\n",
      "256/9308 [..............................] - ETA: 0s - loss: 657290.2500 - mean_squared_error: 657290.2500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "5632/9308 [=================>............] - ETA: 0s - loss: 620551.9489 - mean_squared_error: 620552.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "9308/9308 [==============================] - 0s 11us/sample - loss: 630629.9832 - mean_squared_error: 630629.9375 - val_loss: 555661.7192 - val_mean_squared_error: 555661.6875\n",
      "Step 0metrics\n",
      "Mean Absolute Error: 595.0002061824209\n",
      "Mean Squared Error: 550943.5747763152\n",
      "Root Mean Squared Error: 742.2557340811287\n",
      "Root Mean Squared Log Error: 2.4991011478856926\n",
      "\n",
      "Step 0 end training time:  2020-08-10 07:41:43.270071\n",
      "2020-08-10 07:41:42.260911: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2020-08-10 07:41:42.260951: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2020-08-10 07:41:42.260994: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (kdss-2cd6v-0): /proc/driver/nvidia/version does not exist\n",
      "2020-08-10 07:41:42.261242: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2020-08-10 07:41:42.269136: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095074999 Hz\n",
      "2020-08-10 07:41:42.269511: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f40cf37b50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-08-10 07:41:42.269544: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-08-10 07:41:43.572847: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%logs --url http://kdss-2cd6v-0.kdhs-858ph.thetestingtenant.svc.cluster.local:10001/history/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
