{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cluster    ML Engine\n",
      "------------------  -----------\n",
      "pythonmldl          python\n"
     ]
    }
   ],
   "source": [
    "%attachments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History URL: http://bluedata-195.bdlocal:10001/history/47\n"
     ]
    }
   ],
   "source": [
    "%%pythonmldl\n",
    "\n",
    "\n",
    "print(\"Importing libraries\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "import os\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Start time \n",
    "print(\"Start time: \", datetime.datetime.now())\n",
    "\n",
    "# Project repo path function\n",
    "def ProjectRepo(path):\n",
    "   ProjectRepo = os.popen('bdvcli --get cluster.project_repo').read().rstrip()\n",
    "   return str(ProjectRepo + '/' + path)\n",
    "\n",
    "\n",
    "print(\"Reading in data\")\n",
    "# Reading in dataset table \n",
    "dbName = \"pqyellowtaxi\"\n",
    "df = pd.read_csv(ProjectRepo('demodata.csv'))\n",
    "\n",
    "# Reading in latitude/longitude coordinate lookup table \n",
    "lookupDbName = \"pqlookup\"\n",
    "dflook = pd.read_csv(ProjectRepo('lookup-ipyheader.csv'))\n",
    "print(\"Done reading in data\")\n",
    "\n",
    "\n",
    "# merging dataset and lookup tables on latitudes/coordinates\n",
    "df = pd.merge(df, dflook[[lookupDbName + '.location_i', lookupDbName + '.long', lookupDbName + '.lat']], how='left', left_on=dbName + '.pulocationid', right_on=lookupDbName + '.location_i')\n",
    "df.rename(columns = {(lookupDbName + '.long'):(dbName + '.startstationlongitude')}, inplace = True)\n",
    "df.rename(columns = {(lookupDbName + '.lat'):(dbName + '.startstationlatitude')}, inplace = True)\n",
    "df = pd.merge(df, dflook[[lookupDbName + '.location_i', lookupDbName + '.long', lookupDbName + '.lat']], how='left', left_on=dbName + '.dolocationid', right_on=lookupDbName + '.location_i')\n",
    "df.rename(columns = {(lookupDbName + '.long'):(dbName + '.endstationlongitude')}, inplace = True)\n",
    "df.rename(columns = {(lookupDbName + '.lat'):(dbName + '.endstationlatitude')}, inplace = True)\n",
    "\n",
    "\n",
    "def fullName(colName):\n",
    "    return dbName + '.' + colName\n",
    "\n",
    "# convert string to datetime\n",
    "df[fullName('tpep_pickup_datetime')] = pd.to_datetime(df[fullName('tpep_pickup_datetime')])\n",
    "df[fullName('tpep_dropoff_datetime')] = pd.to_datetime(df[fullName('tpep_dropoff_datetime')])\n",
    "df[fullName('duration')] = (df[fullName(\"tpep_dropoff_datetime\")] - df[fullName(\"tpep_pickup_datetime\")]).dt.total_seconds()\n",
    "\n",
    "# feature engineering\n",
    "df[fullName(\"weekday\")] = (df[fullName('tpep_pickup_datetime')].dt.dayofweek < 5).astype(float)\n",
    "df[fullName(\"hour\")] = df[fullName('tpep_pickup_datetime')].dt.hour\n",
    "df[fullName(\"work\")] = (df[fullName('weekday')] == 1) & (df[fullName(\"hour\")] >= 8) & (df[fullName(\"hour\")] < 18)\n",
    "df[fullName(\"month\")] = df[fullName('tpep_pickup_datetime')].dt.month\n",
    "# convert month to a categorical feature using one-hot encoding\n",
    "df = pd.get_dummies(df, columns=[fullName(\"month\")])\n",
    "\n",
    "# Filter dataset to rides under 3 hours and under 150 miles to remove outliers\n",
    "df = df[df[fullName('duration')] > 20]\n",
    "df = df[df[fullName('duration')] < 10800]\n",
    "df = df[df[fullName('trip_distance')] > 0]\n",
    "df = df[df[fullName('trip_distance')] < 150]\n",
    "\n",
    "# drop null rows\n",
    "df = df.dropna(how='any',axis=0)\n",
    "\n",
    "# select columns to be used as features\n",
    "cols = [fullName('work'), fullName('startstationlatitude'), fullName('startstationlongitude'), fullName('endstationlatitude'), fullName('endstationlongitude'), fullName('trip_distance'), fullName('weekday'), fullName('hour')]\n",
    "cols.extend([fullName('month_' + str(x)) for x in range(1, 7)])\n",
    "cols.append(fullName('duration'))\n",
    "dataset = df[cols]\n",
    "\n",
    "\n",
    "X = dataset.iloc[:, 0:(len(cols) - 1)].values\n",
    "y = dataset.iloc[:, (len(cols) - 1)].values\n",
    "X = X.copy()\n",
    "y = y.copy()\n",
    "del dataset\n",
    "del df\n",
    "\n",
    "print(\"Done cleaning data\")\n",
    "\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "xgbr = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 1, subsample = 1, learning_rate = 0.15, booster = \"gbtree\", max_depth = 15, eta = 0.5, eval_metric = \"rmse\",) \n",
    "print(\"num train elements: \" + str(len(X_train)))\n",
    "print(\"Train start time: \", datetime.datetime.now())\n",
    "xgbr.fit(X_train, y_train)\n",
    "print(\"Train end time: \", datetime.datetime.now())\n",
    "y_pred = xgbr.predict(X_test)\n",
    "y_pred = y_pred.clip(min=0)\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('Root Mean Squared Log Error:', np.sqrt(mean_squared_log_error( y_test, y_pred)))\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Saving model\")\n",
    "pickle.dump(xgbr, open( ProjectRepo('models/') + \"XGB.pickle.dat\", \"wb\"))\n",
    "\n",
    "\n",
    "# from xgboost import plot_importance\n",
    "# plot_importance(xgbr, max_num_features=10) # top 10 most important features\n",
    "# plt.show()\n",
    "\n",
    "# Finish time\n",
    "print(\"End time: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status: Finished\n",
      "Importing libraries\n",
      "Start time:  2020-06-30 16:20:03.975878\n",
      "Reading in data\n",
      "Done reading in data\n",
      "Done cleaning data\n",
      "Training...\n",
      "num train elements: 264325\n",
      "Train start time:  2020-06-30 16:20:08.503259\n",
      "Train end time:  2020-06-30 16:22:08.580873\n",
      "Mean Absolute Error: 177.31845364978025\n",
      "Mean Squared Error: 80833.71930032197\n",
      "Root Mean Squared Error: 284.31271392662336\n",
      "Root Mean Squared Log Error: 0.30597408418927413\n",
      "\n",
      "Saving model\n",
      "End time:  2020-06-30 16:22:12.264221\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%logs --url http://bluedata-195.bdlocal:10001/history/47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
